from nutrient_game import nutrient_game_grid
from DRL_agent import ReplayBuffer, DRL_agent
import numpy as np
import torch

def eval_policy(agent, num_trials, simulation_days):
    environment = nutrient_game_grid(simulation_days)
    avg_reward = 0
    for i in range(num_trials):
        state, reward, _ = environment.reset()
        timestep = 0
        while timestep < simulation_days:
            timestep += 1
            a = agent.select_action(np.array(state)).clip(0.0, 10)
            action = [a[0], a[1], a[2], a[3]]
            reward = environment.take_action(action)
            state = [environment.N, environment.P, environment.K, environment.water]
            avg_reward += reward

    avg_reward /= num_trials
    return avg_reward

def train(environment, agent, cache, simulation_days, num_trials):
    batch_size = 30
    max_timesteps = num_trials*simulation_days
    
    state, reward, terminal = environment.reset()
    
    evaluations = [eval_policy(agent, 15, simulation_days)]
    rewards = []
    a_loss = []
    q_loss = []
    itr = 1
    itr_reward = 0
    itr_timesteps = 0
    rewards_list = []

    for i in range(0, max_timesteps):
        itr_timesteps += 1
        
        #get the action generated by the actor, adding in gaussian noise
        a = agent.select_action(np.array(state) + np.random.normal(0, 5, size=4)).clip(0.0, 10)
        action = [a[0], a[1], a[2], a[3]]
        
        #take the action and get the corresponding reward and next action
        reward = environment.take_action(action)
        next_state = [environment.N, environment.P, environment.K, environment.water]
        terminal = 0 if itr_timesteps == environment.simulation_days else 1

        #store (state, action, reward, next_state, terminal) in the replay buffer
        cache.populate(state, action, reward, next_state, terminal)
        
        itr_reward += reward
        rewards.append(reward)
        state = next_state

        #train after collecting enough samples
        if cache.size > 2*batch_size:
            actor_loss, critic_loss = agent.training(cache, batch_size)
            a_loss.append(actor_loss)
            q_loss.append(critic_loss)
        
        if (i+1)%7 == 0:
            avg_reward = eval_policy(agent, 15, simulation_days)
            evaluations.append(avg_reward)

        if terminal == 0:
            print(f"Finished trial {itr} of {num_trials} total with average reward {itr_reward/simulation_days:.3f}")
            rewards_list.append(itr_reward/simulation_days)
            state, reward, terminal = environment.reset()
            rewards = []
            itr_reward = 0
            itr += 1
            itr_timesteps = 0
        
    return rewards_list

def simulate_nutrients(environment, algorithm, num_trials, simulation_days):
    reward_list = []
    state, reward, terminal = environment.reset()
    
    #random agent: choose a random action at each timestep
    if algorithm == 0:
        reward = 0
        for i in range(num_trials):    
            timestep = 0
            while timestep < environment.simulation_days:
                timestep += 1
                N = np.random.uniform(0, 10)
                P = np.random.uniform(0, 10)
                K = np.random.uniform(0, 10)
                water = np.random.uniform(0, 10)
                action = [N, P, K, water]
                reward += environment.take_action(action)
            environment.reset()
            reward_list.append(reward/environment.simulation_days)
            reward = 0
    
    #hardcoded agent: predetermined sequence of actions
    if algorithm == 1:
        reward = 0
        for i in range(num_trials):
            timestep = 0
            while timestep < environment.simulation_days:
                timestep += 1
                action = [1, 1, 1, 1]
                reward += environment.take_action(action)
            environment.reset()
            reward_list.append(reward/environment.simulation_days)
            reward = 0

    #reactive agent: take action when the resources are close to the threshold
    if algorithm == 2:
        reward = 0
        for i in range(num_trials):
            timestep = 0
            while timestep < environment.simulation_days:
                timestep += 1
                if environment.N < 49:
                    N = np.random.uniform(0, 10)
                else:
                    N = 0
                if environment.P < 31:
                    P = np.random.uniform(0, 10)
                else:
                    P = 0
                if environment.K < 43:
                    K = np.random.uniform(0, 10)
                else:
                    K = 0
                if environment.water < 9:
                    water = np.random.uniform(0, 10)
                else:
                    water = 0
                action = [N, P, K, water]
                reward += environment.take_action(action)
            environment.reset()
            reward_list.append(reward/environment.simulation_days)
            reward = 0
    
    #DRL agent
    if algorithm == 3:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        agent = DRL_agent(10, device)
        cache = ReplayBuffer(device)
        reward_list = train(environment, agent, cache, simulation_days, num_trials)
    
    return reward_list
