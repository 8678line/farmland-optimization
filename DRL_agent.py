from nutrient_game import nutrient_game_grid
import numpy as np
import copy
import torch
import torch.nn.functional as f

'''
Store a cache table of the previous information:
    states
    actions
    corresponding next states
    rewards for this transition
    a "done" indicator to indicate environment simulation termination
'''
class ReplayBuffer():
    def __init__(self, device, max_size = 1000):
        self.max_size = max_size #capacity of this buffer
        self.pointer = 0 #point to the first buffer entry
        self.size = 0 #the running size of the buffer

        self.state = np.zeros((max_size, 4)) #store the current state
        self.action = np.zeros((max_size, 4)) #store the action
        self.reward = np.zeros((max_size, 1)) #store the corresponding reward
        self.next_state = np.zeros((max_size, 4)) #store the corresponding next state
        self.terminal = np.zeros((max_size, 1)) #store the terminal (0: terminal, 1: otherwise)
        self.device = device

    #populate one entry of the buffer
    def populate(self, state, action, reward, next_state, terminal):
        self.state[self.pointer] = state
        self.action[self.pointer] = action
        self.reward[self.pointer] = reward
        self.next_state[self.pointer] = next_state
        self.terminal[self.pointer] = terminal

        #increment the pointer and wraparound upon reaching the end
        self.pointer = (self.pointer + 1) % self.max_size

        #the current number of entries in the buffer
        self.size = min(self.size + 1, self.max_size)

    #sample a batch of cached data from the buffer
    def sample(self, batch_size):
        batch_indices = np.random.choice(self.size, batch_size, replace=False)
        sample_state = torch.FloatTensor(self.state[batch_indices]).to(self.device)
        sample_action = torch.FloatTensor(self.action[batch_indices]).to(self.device)
        sample_reward = torch.FloatTensor(self.reward[batch_indices]).to(self.device)
        sample_next_state = torch.FloatTensor(self.next_state[batch_indices]).to(self.device)
        sample_terminal = torch.FloatTensor(self.terminal[batch_indices]).to(self.device)

        return (sample_state, sample_action, sample_reward, sample_next_state, sample_terminal)

'''
Define the actor network
    this network attempts to generate the optimal next action given the current state
'''
class Actor(torch.nn.Module):
    def __init__(self, max_action):
        super(Actor, self).__init__()
        self.fc1 = torch.nn.Linear(4, 64)
        self.fc2 = torch.nn.Linear(64, 32)
        self.a = torch.nn.Linear(32, 4)
        self.max_action = max_action

    def forward(self, state):
        x = self.fc1(state)
        x = f.relu(x)
        x = self.fc2(x)
        x = f.relu(x)
        action = self.a(x)
        action = torch.tanh(action)
        scaled_action = self.max_action * action
        return scaled_action

'''
Define the critic network
    this network evaluates the action generated by the actor and assigns a q-value to the state-action pair
'''
class Critic(torch.nn.Module):
    def __init__(self):
        super(Critic, self).__init__()
        self.fc1 = torch.nn.Linear(4 + 4, 64)
        self.fc2 = torch.nn.Linear(64, 32)
        self.q = torch.nn.Linear(32, 1)

    def forward(self, state, action):
        x = self.fc1(torch.cat([state, action], 1))
        x = f.relu(x)
        x = self.fc2(x)
        x = f.relu(x)
        q_value = self.q(x)
        return q_value

class DRL_agent(object):
    def __init__(self, max_action, device):
        self.actor = Actor(max_action).to(device)
        self.actor_target = copy.deepcopy(self.actor)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=8e-5)

        self.critic = Critic().to(device)
        self.critic_target = copy.deepcopy(self.critic)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)

        self.discount = 0.99
        self.tau = 0.005
        self.device = device

    def select_action(self, state): 
        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)
        return self.actor(state).data.numpy()[0]

    def training(self, replay_buffer, batch_size):
        #take a batch sample from the replay buffer
        state, action, reward, next_state, terminal = replay_buffer.sample(batch_size)
        
        #training the critic network
        #compute the target Q value using the next observation
        target_Q = self.critic_target(next_state, self.actor_target(next_state))

        #freeze the critic network
        target_Q = reward + (terminal * self.discount * target_Q).detach()
        current_Q = self.critic(state, action)

        #measure the loss between current Q and target Q
        critic_loss = f.mse_loss(current_Q, target_Q)

        #optimize the critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        #train the actor network
        #compute the actor loss
        actor_loss = -self.critic(state, self.actor(state)).mean()

        #optimize the actor 
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        #update the frozen critic network
        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

        return actor_loss.item(), critic_loss.item()